Based on the provided text, Q-learning is a model-free, off-policy algorithm used in reinforcement learning.  It was extended and integrated from prior work in reinforcement learning research in 1989 by Chris Watkins.  It's an off-policy algorithm, meaning its behavior policy doesn't need to be the same as its target policy.  In Deep Q-Networks (DQN), Q-learning, combined with experience replay, was used to learn from experiences by interacting with a game emulator.  Experience replay stores experiences in a memory that is accessed to perform weight updates, allowing for more efficient learning and reduced variance.