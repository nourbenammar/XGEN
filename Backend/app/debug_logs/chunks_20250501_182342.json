{
  "7023": {
    "text": "Bell-\nman optimality equation (4.1) holds, and thus that the policy and the value function are\noptimal. The evaluation and improvement processes in GPI can be viewed as both competing\nand cooperating. They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically makes the value\nfunction incorrect for the changed policy, and making the value function consistent with\nthe policy typically causes that policy no longer to be greedy. In the long run, however,\n\n4.7. E\ufb03ciency of Dynamic Programming\n87\nthese two processes interact to \ufb01nd a single joint solution: the optimal value function\nand an optimal policy. v\u21e4, \u21e1\u21e4\n\u21e1 = greedy(v)\nv, \u21e1\nv = v\u21e1\nOne might also think of the interaction between\nthe evaluation and improvement processes in GPI\nin terms of two constraints or goals\u2014for example,\nas two lines in two-dimensional space as suggested\nby the diagram to the right. Although the real\ngeometry is much more complicated than this, the\ndiagram suggests what happens in the real case. Each process drives the value function or policy\ntoward one of the lines representing a solution to\none of the two goals. The goals interact because the two lines are not orthogonal. Driving\ndirectly toward one goal causes some movement away from the other goal. Inevitably,\nhowever, the joint process is brought closer to the overall goal of optimality. The arrows\nin this diagram correspond to the behavior of policy iteration in that each takes the\nsystem all the way to achieving one of the two goals completely. In GPI one could\nalso take smaller, incomplete steps toward each goal. In either case, the two processes\ntogether achieve the overall goal of optimality even though neither is attempting to\nachieve it directly. 4.7\nE\ufb03ciency of Dynamic Programming\nDP may not be practical for very large problems, but compared with other methods\nfor solving MDPs, DP methods are actually quite e\ufb03cient.",
    "chapter_id": 140,
    "score": 0.5561941266059875,
    "page": 106
  },
  "7066": {
    "text": "e state set (we explore this further in\nChapter 8). A fourth advantage of Monte Carlo methods, which we discuss later in the book, is\nthat they may be less harmed by violations of the Markov property. This is because\nthey do not update their value estimates on the basis of the value estimates of successor\nstates. In other words, it is because they do not bootstrap. In designing Monte Carlo control methods we have followed the overall schema of\ngeneralized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting\nprocesses of policy evaluation and policy improvement. Monte Carlo methods provide\nan alternative policy evaluation process. Rather than use a model to compute the value\nof each state, they simply average many returns that start in the state. Because a state\u2019s\nvalue is the expected return, this average can become a good approximation to the\nvalue. In control methods we are particularly interested in approximating action-value\nfunctions, because these can be used to improve the policy without requiring a model of\nthe environment\u2019s transition dynamics. Monte Carlo methods intermix policy evaluation\nand policy improvement steps on an episode-by-episode basis, and can be incrementally\nimplemented on an episode-by-episode basis. Maintaining su\ufb03cient exploration is an issue in Monte Carlo control methods. It is\nnot enough just to select the actions currently estimated to be best, because then no\nreturns will be obtained for alternative actions, and it may never be learned that they are\nactually better. One approach is to ignore this problem by assuming that episodes begin\nwith state\u2013action pairs randomly selected to cover all possibilities. Such exploring starts\ncan sometimes be arranged in applications with simulated episodes, but are unlikely\nin learning from real experience. In on-policy methods, the agent commits to always\nexploring and tries to \ufb01nd the best policy that still explores.",
    "chapter_id": 141,
    "score": 0.5494959354400635,
    "page": 136
  },
  "7025": {
    "text": "mputers to solve MDPs with\nmillions of states. Both policy iteration and value iteration are widely used, and it is not\n\n88\nChapter 4: Dynamic Programming\nclear which, if either, is better in general. In practice, these methods usually converge\nmuch faster than their theoretical worst-case run times, particularly if they are started\nwith good initial value functions or policies. On problems with large state spaces, asynchronous DP methods are often preferred. To complete even one sweep of a synchronous method requires computation and mem-\nory for every state. For some problems, even this much memory and computation is\nimpractical, yet the problem is still potentially solvable because relatively few states\noccur along optimal solution trajectories. Asynchronous methods and other variations\nof GPI can be applied in such cases and may \ufb01nd good or optimal policies much faster\nthan synchronous methods can. 4.8\nSummary\nIn this chapter we have become familiar with the basic ideas and algorithms of dynamic\nprogramming as they relate to solving \ufb01nite MDPs. Policy evaluation refers to the (typi-\ncally) iterative computation of the value functions for a given policy. Policy improvement\nrefers to the computation of an improved policy given the value function for that policy. Putting these two computations together, we obtain policy iteration and value iteration,\nthe two most popular DP methods. Either of these can be used to reliably compute\noptimal policies and value functions for \ufb01nite MDPs given complete knowledge of the\nMDP. Classical DP methods operate in sweeps through the state set, performing an expected\nupdate operation on each state. Each such operation updates the value of one state based\non the values of all possible successor states and their probabilities of occurring. Ex-\npected updates are closely related to Bellman equations: they are little more than these\nequations turned into assignment statements.",
    "chapter_id": 140,
    "score": 0.5169194340705872,
    "page": 107
  },
  "6914": {
    "text": "s. Also contributing may have been the preva-\nlent view of dynamic programming as an o\ufb00-line computation depending essentially on\naccurate system models and analytic solutions to the Bellman equation. Further, the\nsimplest form of dynamic programming is a computation that proceeds backwards in\ntime, making it di\ufb03cult to see how it could be involved in a learning process that must\nproceed in a forward direction. Some of the earliest work in dynamic programming,\nsuch as that by Bellman and Dreyfus (1959), might now be classi\ufb01ed as following a\nlearning approach. Witten\u2019s (1977) work (discussed below) certainly quali\ufb01es as a com-\nbination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly\n\n1.7. Early History of Reinforcement Learning\n15\nfor greater interrelation of dynamic programming and learning methods and its rele-\nvance to understanding neural and cognitive mechanisms. For us the full integration\nof dynamic programming methods with on-line learning did not occur until the work\nof Chris Watkins in 1989, whose treatment of reinforcement learning using the MDP\nformalism has been widely adopted. Since then these relationships have been exten-\nsively developed by many researchers, most particularly by Dimitri Bertsekas and John\nTsitsiklis (1996), who coined the term \u201cneurodynamic programming\u201d to refer to the\ncombination of dynamic programming and neural networks. Another term currently in\nuse is \u201capproximate dynamic programming.\u201d These various approaches emphasize dif-\nferent aspects of the subject, but they all share with reinforcement learning an interest\nin circumventing the classical shortcomings of dynamic programming. We would consider all of the work in optimal control also to be, in a sense, work\nin reinforcement learning.",
    "chapter_id": 137,
    "score": 0.5096596479415894,
    "page": 35
  },
  "6903": {
    "text": "playing by the opponent. For example, a minimax player would never reach a game state from which it could\nlose, even if in fact it always won from that state because of incorrect play by the\nopponent. Classical optimization methods for sequential decision problems, such as\ndynamic programming, can compute an optimal solution for any opponent, but require\nas input a complete speci\ufb01cation of that opponent, including the probabilities with which\nthe opponent makes each move in each board state. Let us assume that this information\n\n1.5. An Extended Example: Tic-Tac-Toe\n9\nis not available a priori for this problem, as it is not for the vast majority of problems of\npractical interest. On the other hand, such information can be estimated from experience,\nin this case by playing many games against the opponent. About the best one can do\non this problem is \ufb01rst to learn a model of the opponent\u2019s behavior, up to some level of\ncon\ufb01dence, and then apply dynamic programming to compute an optimal solution given\nthe approximate opponent model. In the end, this is not that di\ufb00erent from some of the\nreinforcement learning methods we examine later in this book. An evolutionary method applied to this problem would directly search the space of\npossible policies for one with a high probability of winning against the opponent. Here,\na policy is a rule that tells the player what move to make for every state of the game\u2014\nevery possible con\ufb01guration of Xs and Os on the three-by-three board. For each policy\nconsidered, an estimate of its winning probability would be obtained by playing some\nnumber of games against the opponent. This evaluation would then direct which policy\nor policies were considered next. A typical evolutionary method would hill-climb in\npolicy space, successively generating and evaluating policies in an attempt to obtain\nincremental improvements. Or, perhaps, a genetic-style algorithm could be used that\nwould maintain and evaluate a population of policies.",
    "chapter_id": 137,
    "score": 0.4998462200164795,
    "page": 29
  }
}