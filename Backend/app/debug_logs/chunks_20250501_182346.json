{
  "6889": {
    "text": "2\nChapter 1: Introduction\nreward but also the next situation and, through that, all subsequent rewards. These two\ncharacteristics\u2014trial-and-error search and delayed reward\u2014are the two most important\ndistinguishing features of reinforcement learning. Reinforcement learning, like many topics whose names end with \u201cing,\u201d such as machine\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\nthat work well on the problem, and the \ufb01eld that studies this problem and its solution\nmethods. It is convenient to use a single name for all three things, but at the same time\nessential to keep the three conceptually separate. In particular, the distinction between\nproblems and solution methods is very important in reinforcement learning; failing to\nmake this distinction is the source of many confusions. We formalize the problem of reinforcement learning using ideas from dynamical sys-\ntems theory, speci\ufb01cally, as the optimal control of incompletely-known Markov decision\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\nis simply to capture the most important aspects of the real problem facing a learning\nagent interacting over time with its environment to achieve a goal. A learning agent\nmust be able to sense the state of its environment to some extent and must be able to\ntake actions that a\ufb00ect the state. The agent also must have a goal or goals relating to\nthe state of the environment. Markov decision processes are intended to include just\nthese three aspects\u2014sensation, action, and goal\u2014in their simplest possible forms with-\nout trivializing any of them. Any method that is well suited to solving such problems\nwe consider to be a reinforcement learning method. Reinforcement learning is di\ufb00erent from supervised learning, the kind of learning stud-\nied in most current research in the \ufb01eld of machine learning.",
    "chapter_id": 137,
    "score": 0.6603752970695496,
    "page": 22
  },
  "6899": {
    "text": "pose of estimating\nvalues is to achieve more reward. Nevertheless, it is values with which we are most\nconcerned when making and evaluating decisions. Action choices are made based on\nvalue judgments. We seek actions that bring about states of highest value, not high-\nest reward, because these actions obtain the greatest amount of reward for us over the\nlong run. Unfortunately, it is much harder to determine values than it is to determine\n\n1.4. Limitations and Scope\n7\nrewards. Rewards are basically given directly by the environment, but values must be\nestimated and re-estimated from the sequences of observations an agent makes over its\nentire lifetime. In fact, the most important component of almost all reinforcement learn-\ning algorithms we consider is a method for e\ufb03ciently estimating values. The central role\nof value estimation is arguably the most important thing that has been learned about\nreinforcement learning over the last six decades. The fourth and \ufb01nal element of some reinforcement learning systems is a model of the\nenvironment. This is something that mimics the behavior of the environment, or more\ngenerally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state\nand next reward. Models are used for planning, by which we mean any way of deciding\non a course of action by considering possible future situations before they are actually\nexperienced. Methods for solving reinforcement learning problems that use models and\nplanning are called model-based methods, as opposed to simpler model-free methods\nthat are explicitly trial-and-error learners\u2014viewed as almost the opposite of planning. In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial\nand error, learn a model of the environment, and use the model for planning.",
    "chapter_id": 137,
    "score": 0.6530355215072632,
    "page": 27
  },
  "6893": {
    "text": " agent can also be a component of a larger behaving system. In this case,\nthe agent directly interacts with the rest of the larger system and indirectly interacts\nwith the larger system\u2019s environment. A simple example is an agent that monitors the\ncharge level of robot\u2019s battery and sends commands to the robot\u2019s control architecture. 4\nChapter 1: Introduction\nThis agent\u2019s environment is the rest of the robot together with the robot\u2019s environment. One must look beyond the most obvious examples of agents and their environments to\nappreciate the generality of the reinforcement learning framework. One of the most exciting aspects of modern reinforcement learning is its substantive\nand fruitful interactions with other engineering and scienti\ufb01c disciplines. Reinforcement\nlearning is part of a decades-long trend within arti\ufb01cial intelligence and machine learn-\ning toward greater integration with statistics, optimization, and other mathematical\nsubjects. For example, the ability of some reinforcement learning methods to learn with\nparameterized approximators addresses the classical \u201ccurse of dimensionality\u201d in oper-\nations research and control theory. More distinctively, reinforcement learning has also\ninteracted strongly with psychology and neuroscience, with substantial bene\ufb01ts going\nboth ways. Of all the forms of machine learning, reinforcement learning is the clos-\nest to the kind of learning that humans and other animals do, and many of the core\nalgorithms of reinforcement learning were originally inspired by biological learning sys-\ntems. Reinforcement learning has also given back, both through a psychological model\nof animal learning that better matches some of the empirical data, and through an in-\n\ufb02uential model of parts of the brain\u2019s reward system. The body of this book develops\nthe ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters 14 and 15.",
    "chapter_id": 137,
    "score": 0.6515374183654785,
    "page": 24
  },
  "6907": {
    "text": " policies,\nbut learning a value function takes advantage of information available during the course\nof play. This simple example illustrates some of the key features of reinforcement learning\nmethods. First, there is the emphasis on learning while interacting with an environment,\nin this case with an opponent player. Second, there is a clear goal, and correct behavior\nrequires planning or foresight that takes into account delayed e\ufb00ects of one\u2019s choices. For example, the simple reinforcement learning player would learn to set up multi-move\ntraps for a shortsighted opponent. It is a striking feature of the reinforcement learning\nsolution that it can achieve the e\ufb00ects of planning and lookahead without using a model\nof the opponent and without conducting an explicit search over possible sequences of\nfuture states and actions. While this example illustrates some of the key features of reinforcement learning, it is\nso simple that it might give the impression that reinforcement learning is more limited\nthan it really is. Although tic-tac-toe is a two-person game, reinforcement learning also\napplies in the case in which there is no external adversary, that is, in the case of a\n\u201cgame against nature.\u201d\nReinforcement learning also is not restricted to problems in\nwhich behavior breaks down into separate episodes, like the separate games of tic-tac-\ntoe, with reward only at the end of each episode. It is just as applicable when behavior\ncontinues inde\ufb01nitely and when rewards of various magnitudes can be received at any\ntime. Reinforcement learning is also applicable to problems that do not even break\ndown into discrete time steps, like the plays of tic-tac-toe. The general principles apply\nto continuous-time problems as well, although the theory gets more complicated and we\nomit it from this introductory treatment. Tic-tac-toe has a relatively small, \ufb01nite state set, whereas reinforcement learning can\nbe used when the state set is very large, or even in\ufb01nite.",
    "chapter_id": 137,
    "score": 0.6458492279052734,
    "page": 31
  },
  "6915": {
    "text": "nt learning an interest\nin circumventing the classical shortcomings of dynamic programming. We would consider all of the work in optimal control also to be, in a sense, work\nin reinforcement learning. We de\ufb01ne a reinforcement learning method as any e\ufb00ective\nway of solving reinforcement learning problems, and it is now clear that these problems\nare closely related to optimal control problems, particularly stochastic optimal control\nproblems such as those formulated as MDPs. Accordingly, we must consider the solution\nmethods of optimal control, such as dynamic programming, also to be reinforcement\nlearning methods. Because almost all of the conventional methods require complete\nknowledge of the system to be controlled, it feels a little unnatural to say that they\nare part of reinforcement learning. On the other hand, many dynamic programming\nalgorithms are incremental and iterative. Like learning methods, they gradually reach\nthe correct answer through successive approximations. As we show in the rest of this\nbook, these similarities are far more than super\ufb01cial. The theories and solution methods\nfor the cases of complete and incomplete knowledge are so closely related that we feel\nthey must be considered together as part of the same subject matter. Let us return now to the other major thread leading to the modern \ufb01eld of reinforce-\nment learning, that centered on the idea of trial-and-error learning. We only touch on\nthe major points of contact here, taking up this topic in more detail in Section 14.3. According to American psychologist R. S. Woodworth the idea of trial-and-error learn-\ning goes as far back as the 1850s to Alexander Bain\u2019s discussion of learning by \u201cgroping\nand experiment\u201d and more explicitly to the British ethologist and psychologist Conway\nLloyd Morgan\u2019s 1894 use of the term to describe his observations of animal behavior\n(Woodworth, 1938).",
    "chapter_id": 137,
    "score": 0.6458145380020142,
    "page": 35
  }
}