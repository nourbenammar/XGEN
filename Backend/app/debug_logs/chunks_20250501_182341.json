{
  "7443": {
    "text": "4.4\nDelayed Reinforcement\nThe Law of E\ufb00ect requires a backward e\ufb00ect on connections, and some early critics of\nthe law could not conceive of how the present could a\ufb00ect something that was in the\npast. This concern was ampli\ufb01ed by the fact that learning can even occur when there\nis a considerable delay between an action and the consequent reward or penalty. Simi-\nlarly, in classical conditioning, learning can occur when US onset follows CS o\ufb00set by a\nnon-negligible time interval. We call this the problem of delayed reinforcement, which\nis related to what Minsky (1961) called the \u201ccredit-assignment problem for learning sys-\ntems\u201d: how do you distribute credit for success among the many decisions that may have\nbeen involved in producing it? The reinforcement learning algorithms presented in this\nbook include two basic mechanisms for addressing this problem. The \ufb01rst is the use of\neligibility traces, and the second is the use of TD methods to learn value functions that\nprovide nearly immediate evaluations of actions (in tasks like instrumental conditioning\nexperiments) or that provide immediate prediction targets (in tasks like classical condi-\ntioning experiments). Both of these methods correspond to similar mechanisms proposed\nin theories of animal learning. Pavlov (1927) pointed out that every stimulus must leave a trace in the nervous system\nthat persists for some time after the stimulus ends, and he proposed that stimulus traces\nmake learning possible when there is a temporal gap between the CS o\ufb00set and the\nUS onset. To this day, conditioning under these conditions is called trace conditioning\n(page 348). Assuming a trace of the CS remains when the US arrives, learning occurs\nthrough the simultaneous presence of the trace and the US. We discuss some proposals\nfor trace mechanisms in the nervous system in Chapter 15.",
    "chapter_id": 150,
    "score": 0.6157437562942505,
    "page": 385
  },
  "6929": {
    "text": "ing a distinct\nearly contribution to temporal-di\ufb00erence learning. The temporal-di\ufb00erence and optimal control threads were fully brought together in\n1989 with Chris Watkins\u2019s development of Q-learning. This work extended and inte-\ngrated prior work in all three threads of reinforcement learning research. Paul Werbos\n(1987) contributed to this integration by arguing for the convergence of trial-and-error\nlearning and dynamic programming since 1977. By the time of Watkins\u2019s work there\nhad been tremendous growth in reinforcement learning research, primarily in the ma-\nchine learning sub\ufb01eld of arti\ufb01cial intelligence, but also in neural networks and arti\ufb01cial\nintelligence more broadly. In 1992, the remarkable success of Gerry Tesauro\u2019s backgam-\nmon playing program, TD-Gammon, brought additional attention to the \ufb01eld. In the time since publication of the \ufb01rst edition of this book, a \ufb02ourishing sub\ufb01eld of\nneuroscience developed that focuses on the relationship between reinforcement learning\nalgorithms and reinforcement learning in the nervous system. Most responsible for this\n\n22\nChapter 1: Introduction\nis an uncanny similarity between the behavior of temporal-di\ufb00erence algorithms and\nthe activity of dopamine producing neurons in the brain, as pointed out by a number of\nresearchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\nDayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\nprovides an introduction to this exciting aspect of reinforcement learning. Other important contributions made in the recent history of reinforcement learning\nare too numerous to mention in this brief account; we cite many of these at the end of\nthe individual chapters in which they arise. Bibliographical Remarks\nFor additional general coverage of reinforcement learning, we refer the reader to the\nbooks by Szepesv\u00b4ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\nSugiyama, Hachiya, and Morimura (2013).",
    "chapter_id": 137,
    "score": 0.5853715538978577,
    "page": 43
  },
  "6926": {
    "text": "aller and less distinct than the other two, but it has played a particularly\nimportant role in the \ufb01eld, in part because temporal-di\ufb00erence methods seem to be new\nand unique to reinforcement learning. The origins of temporal-di\ufb00erence learning are in part in animal learning psychology,\nin particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulus\nthat has been paired with a primary reinforcer such as food or pain and, as a result, has\ncome to take on similar reinforcing properties. Minsky (1954) may have been the \ufb01rst to\nrealize that this psychological principle could be important for arti\ufb01cial learning systems. Arthur Samuel (1959) was the \ufb01rst to propose and implement a learning method that\nincluded temporal-di\ufb00erence ideas, as part of his celebrated checkers-playing program. Samuel made no reference to Minsky\u2019s work or to possible connections to animal\nlearning. His inspiration apparently came from Claude Shannon\u2019s (1950) suggestion that\na computer could be programmed to use an evaluation function to play chess, and that it\nmight be able to improve its play by modifying this function on-line. (It is possible that\nthese ideas of Shannon\u2019s also in\ufb02uenced Bellman, but we know of no evidence for this.) Minsky (1961) extensively discussed Samuel\u2019s work in his \u201cSteps\u201d paper, suggesting the\nconnection to secondary reinforcement theories, both natural and arti\ufb01cial. As we have discussed, in the decade following the work of Minsky and Samuel, little\ncomputational work was done on trial-and-error learning, and apparently no computa-\ntional work at all was done on temporal-di\ufb00erence learning. In 1972, Klopf brought\ntrial-and-error learning together with an important component of temporal-di\ufb00erence\nlearning. Klopf was interested in principles that would scale to learning in large systems,\nand thus was intrigued by notions of local reinforcement, whereby subcomponents of an\noverall learning system could reinforce one another.",
    "chapter_id": 137,
    "score": 0.5701520442962646,
    "page": 41
  },
  "7458": {
    "text": "he key di\ufb00erence between in-\nstrumental and classical conditioning experiments is that in the former the reinforcing\nstimulus is contingent upon the animal\u2019s behavior, whereas in the latter it is not. Learn-\ning to predict via a TD algorithm corresponds to classical conditioning, and we described\nthe TD model of classical conditioning as one instance in which reinforcement learning\nprinciples account for some details of animal learning behavior. This model general-\nizes the in\ufb02uential Rescorla\u2013Wagner model by including the temporal dimension where\nevents within individual trials in\ufb02uence learning, and it provides an account of second-\norder conditioning, where predictors of reinforcing stimuli become reinforcing themselves. It also is the basis of an in\ufb02uential view of the activity of dopamine neurons in the brain,\nsomething we take up in Chapter 15. Learning by trial and error is at the base of the control aspect of reinforcement learning. We presented some details about Thorndike\u2019s experiments with cats and other animals\n\n374\nChapter 14: Psychology\nthat led to his Law of E\ufb00ect, which we discussed here and in Chapter 1. We pointed\nout that in reinforcement learning, exploration does not have to be limited to \u201cblind\ngroping\u201d; trials can be generated by sophisticated methods using innate and previously\nlearned knowledge as long as there is some exploration. We discussed the training method\nB. F. Skinner called shaping in which reward contingencies are progressively altered to\ntrain an animal to successively approximate a desired behavior. Shaping is not only\nindispensable for animal training, it is also an e\ufb00ective tool for training reinforcement\nlearning agents. There is also a connection to the idea of an animal\u2019s motivational state,\nwhich in\ufb02uences what an animal will approach or avoid and what events are rewarding\nor punishing for the animal.",
    "chapter_id": 150,
    "score": 0.5303680896759033,
    "page": 393
  },
  "7520": {
    "text": "apses occur only if a neuromodulatory pulse arrives within a time window\nthat can last up to 10 seconds after a presynaptic spike is closely followed by a postsy-\nnaptic spike (Yagishita et al. 2014). Although the evidence is indirect, these experiments\npoint to the existence of contingent eligibility traces having prolonged time courses. The\nmolecular mechanisms producing these traces, as well as the much shorter traces that\nlikely underly STDP, are not yet understood, but research focusing on time-dependent\nand neuromodulator-dependent synaptic plasticity is continuing. The neuron-like actor unit that we have described here, with its Law-of-E\ufb00ect-style\nlearning rule, appeared in somewhat simpler form in the actor\u2013critic network of Barto et\nal. (1983). That network was inspired by the \u201chedonistic neuron\u201d hypothesis proposed\nby physiologist A. H. Klopf (1972, 1982). Not all the details of Klopf\u2019s hypothesis are\nconsistent with what has been learned about synaptic plasticity, but the discovery of\nSTDP and the growing evidence for a reward-modulated form of STDP suggest that\nKlopf\u2019s ideas may not have been far o\ufb00 the mark. We discuss Klopf\u2019s hedonistic neuron\nhypothesis next. 15.9\nHedonistic Neurons\nIn his hedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual neu-\nrons seek to maximize the di\ufb00erence between synaptic input treated as rewarding and\nsynaptic input treated as punishing by adjusting the e\ufb03cacies of their synapses on the\nbasis of rewarding or punishing consequences of their own action potentials. In other\nwords, individual neurons can be trained with response-contingent reinforcement like an\nanimal can be trained in an instrumental conditioning task. His hypothesis included\nthe idea that rewards and punishments are conveyed to a neuron via the same synaptic\ninput that excites or inhibits the neuron\u2019s spike-generating activity.",
    "chapter_id": 151,
    "score": 0.5260039567947388,
    "page": 426
  }
}