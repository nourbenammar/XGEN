{
  "6889": {
    "text": "2\nChapter 1: Introduction\nreward but also the next situation and, through that, all subsequent rewards. These two\ncharacteristics\u2014trial-and-error search and delayed reward\u2014are the two most important\ndistinguishing features of reinforcement learning. Reinforcement learning, like many topics whose names end with \u201cing,\u201d such as machine\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\nthat work well on the problem, and the \ufb01eld that studies this problem and its solution\nmethods. It is convenient to use a single name for all three things, but at the same time\nessential to keep the three conceptually separate. In particular, the distinction between\nproblems and solution methods is very important in reinforcement learning; failing to\nmake this distinction is the source of many confusions. We formalize the problem of reinforcement learning using ideas from dynamical sys-\ntems theory, speci\ufb01cally, as the optimal control of incompletely-known Markov decision\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\nis simply to capture the most important aspects of the real problem facing a learning\nagent interacting over time with its environment to achieve a goal. A learning agent\nmust be able to sense the state of its environment to some extent and must be able to\ntake actions that a\ufb00ect the state. The agent also must have a goal or goals relating to\nthe state of the environment. Markov decision processes are intended to include just\nthese three aspects\u2014sensation, action, and goal\u2014in their simplest possible forms with-\nout trivializing any of them. Any method that is well suited to solving such problems\nwe consider to be a reinforcement learning method. Reinforcement learning is di\ufb00erent from supervised learning, the kind of learning stud-\nied in most current research in the \ufb01eld of machine learning.",
    "chapter_id": 137,
    "score": 0.6841793060302734,
    "page": 22
  },
  "6893": {
    "text": " agent can also be a component of a larger behaving system. In this case,\nthe agent directly interacts with the rest of the larger system and indirectly interacts\nwith the larger system\u2019s environment. A simple example is an agent that monitors the\ncharge level of robot\u2019s battery and sends commands to the robot\u2019s control architecture. 4\nChapter 1: Introduction\nThis agent\u2019s environment is the rest of the robot together with the robot\u2019s environment. One must look beyond the most obvious examples of agents and their environments to\nappreciate the generality of the reinforcement learning framework. One of the most exciting aspects of modern reinforcement learning is its substantive\nand fruitful interactions with other engineering and scienti\ufb01c disciplines. Reinforcement\nlearning is part of a decades-long trend within arti\ufb01cial intelligence and machine learn-\ning toward greater integration with statistics, optimization, and other mathematical\nsubjects. For example, the ability of some reinforcement learning methods to learn with\nparameterized approximators addresses the classical \u201ccurse of dimensionality\u201d in oper-\nations research and control theory. More distinctively, reinforcement learning has also\ninteracted strongly with psychology and neuroscience, with substantial bene\ufb01ts going\nboth ways. Of all the forms of machine learning, reinforcement learning is the clos-\nest to the kind of learning that humans and other animals do, and many of the core\nalgorithms of reinforcement learning were originally inspired by biological learning sys-\ntems. Reinforcement learning has also given back, both through a psychological model\nof animal learning that better matches some of the empirical data, and through an in-\n\ufb02uential model of parts of the brain\u2019s reward system. The body of this book develops\nthe ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters 14 and 15.",
    "chapter_id": 137,
    "score": 0.6345391273498535,
    "page": 24
  },
  "6900": {
    "text": " almost the opposite of planning. In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial\nand error, learn a model of the environment, and use the model for planning. Modern\nreinforcement learning spans the spectrum from low-level, trial-and-error learning to\nhigh-level, deliberative planning. 1.4\nLimitations and Scope\nReinforcement learning relies heavily on the concept of state\u2014as input to the policy and\nvalue function, and as both input to and output from the model. Informally, we can\nthink of the state as a signal conveying to the agent some sense of \u201chow the environment\nis\u201d at a particular time. The formal de\ufb01nition of state as we use it here is given by\nthe framework of Markov decision processes presented in Chapter 3. More generally,\nhowever, we encourage the reader to follow the informal meaning and think of the state\nas whatever information is available to the agent about its environment. In e\ufb00ect, we\nassume that the state signal is produced by some preprocessing system that is nominally\npart of the agent\u2019s environment. We do not address the issues of constructing, changing,\nor learning the state signal in this book (other than brie\ufb02y in Section 17.3). We take\nthis approach not because we consider state representation to be unimportant, but in\norder to focus fully on the decision-making issues. In other words, our main concern is\nnot with designing the state signal, but with deciding what action to take as a function\nof whatever state signal is available. Most of the reinforcement learning methods we consider in this book are structured\naround estimating value functions, but it is not strictly necessary to do this to solve rein-\nforcement learning problems. For example, solution methods such as genetic algorithms,\ngenetic programming, simulated annealing, and other optimization methods never esti-\nmate value functions.",
    "chapter_id": 137,
    "score": 0.5759490728378296,
    "page": 27
  },
  "6961": {
    "text": "kinner, 1938), a discriminative stimulus is a stimulus that signals the presence\nof a particular reinforcement contingency. In our terms, di\ufb00erent discriminative\nstimuli correspond to di\ufb00erent states. 2.10\nBellman (1956) was the \ufb01rst to show how dynamic programming could be used\nto compute the optimal balance between exploration and exploitation within\na Bayesian formulation of the problem. The Gittins index approach is due to\nGittins and Jones (1974). Du\ufb00 (1995) showed how it is possible to learn Gittins\nindices for bandit problems through reinforcement learning. The survey by Ku-\nmar (1985) provides a good discussion of Bayesian and non-Bayesian approaches\nto these problems. The term information state comes from the literature on par-\ntially observable MDPs; see, e.g., Lovejoy (1991). Other theoretical research focuses on the e\ufb03ciency of exploration, usually ex-\npressed as how quickly an algorithm can approach an optimal decision-making\npolicy. One way to formalize exploration e\ufb03ciency is by adapting to reinforce-\nment learning the notion of sample complexity for a supervised learning algo-\nrithm, which is the number of training examples the algorithm needs to attain\na desired degree of accuracy in learning the target function. A de\ufb01nition of\nthe sample complexity of exploration for a reinforcement learning algorithm is\nthe number of time steps in which the algorithm does not select near-optimal\nactions (Kakade, 2003). Li (2012) discusses this and several other approaches\nin a survey of theoretical approaches to exploration e\ufb03ciency in reinforcement\nlearning.",
    "chapter_id": 138,
    "score": 0.5473018884658813,
    "page": 66
  },
  "7294": {
    "text": " 1;\nsee Chapter 12) but often bootstrapping greatly increases e\ufb03ciency. It is an ability that\nwe would very much like to keep in our toolkit. Finally, there is o\ufb00-policy learning; can we give that up? On-policy methods are\noften adequate. For model-free reinforcement learning, one can simply use Sarsa rather\nthan Q-learning. O\ufb00-policy methods free behavior from the target policy. This could be\nconsidered an appealing convenience but not a necessity. However, o\ufb00-policy learning is\nessential to other anticipated use cases, cases that we have not yet mentioned in this\nbook but may be important to the larger goal of creating a powerful intelligent agent. In these use cases, the agent learns not just a single value function and single policy,\nbut large numbers of them in parallel. There is extensive psychological evidence that\npeople and animals learn to predict many di\ufb00erent sensory events, not just rewards. We can be surprised by unusual events, and correct our predictions about them, even if\nthey are of neutral valence (neither good nor bad). This kind of prediction presumably\nunderlies predictive models of the world such as are used in planning. We predict what\nwe will see after eye movements, how long it will take to walk home, the probability of\nmaking a jump shot in basketball, and the satisfaction we will get from taking on a new\nproject. In all these cases, the events we would like to predict depend on our acting\nin a certain way. To learn them all, in parallel, requires learning from the one stream\nof experience. There are many target policies, and thus the one behavior policy cannot\nequal all of them. Yet parallel learning is conceptually possible because the behavior\npolicy may overlap in part with many of the target policies. To take full advantage of\nthis requires o\ufb00-policy learning.",
    "chapter_id": 147,
    "score": 0.5406380295753479,
    "page": 286
  }
}