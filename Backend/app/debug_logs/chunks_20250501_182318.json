{
  "6889": {
    "text": "2\nChapter 1: Introduction\nreward but also the next situation and, through that, all subsequent rewards. These two\ncharacteristics\u2014trial-and-error search and delayed reward\u2014are the two most important\ndistinguishing features of reinforcement learning. Reinforcement learning, like many topics whose names end with \u201cing,\u201d such as machine\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\nthat work well on the problem, and the \ufb01eld that studies this problem and its solution\nmethods. It is convenient to use a single name for all three things, but at the same time\nessential to keep the three conceptually separate. In particular, the distinction between\nproblems and solution methods is very important in reinforcement learning; failing to\nmake this distinction is the source of many confusions. We formalize the problem of reinforcement learning using ideas from dynamical sys-\ntems theory, speci\ufb01cally, as the optimal control of incompletely-known Markov decision\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\nis simply to capture the most important aspects of the real problem facing a learning\nagent interacting over time with its environment to achieve a goal. A learning agent\nmust be able to sense the state of its environment to some extent and must be able to\ntake actions that a\ufb00ect the state. The agent also must have a goal or goals relating to\nthe state of the environment. Markov decision processes are intended to include just\nthese three aspects\u2014sensation, action, and goal\u2014in their simplest possible forms with-\nout trivializing any of them. Any method that is well suited to solving such problems\nwe consider to be a reinforcement learning method. Reinforcement learning is di\ufb00erent from supervised learning, the kind of learning stud-\nied in most current research in the \ufb01eld of machine learning.",
    "chapter_id": 137,
    "score": 0.6841793060302734,
    "page": 22
  },
  "7472": {
    "text": "cation therapy. Of course, both of these directions of control are at play when an agent interacts with\nits environment, but our focus is on the agent as controller; not the environment as\ncontroller. A view equivalent to ours, and perhaps more illuminating, is that the agent\nis actually controlling the input it receives from its environment (Powers, 1973). This is\nnot what psychologists mean by stimulus control. Sometimes reinforcement learning is understood to refer solely to learning policies\ndirectly from rewards (and penalties) without the involvement of value functions or en-\nvironment models. This is what psychologists call stimulus-response, or S-R, learning. But for us, along with most of today\u2019s psychologists, reinforcement learning is much\nbroader than this, including in addition to S-R learning, methods involving value func-\ntions, environment models, planning, and other processes that are commonly thought to\nbelong to the more cognitive side of mental functioning.",
    "chapter_id": 150,
    "score": 0.6417410373687744,
    "page": 400
  },
  "6893": {
    "text": " agent can also be a component of a larger behaving system. In this case,\nthe agent directly interacts with the rest of the larger system and indirectly interacts\nwith the larger system\u2019s environment. A simple example is an agent that monitors the\ncharge level of robot\u2019s battery and sends commands to the robot\u2019s control architecture. 4\nChapter 1: Introduction\nThis agent\u2019s environment is the rest of the robot together with the robot\u2019s environment. One must look beyond the most obvious examples of agents and their environments to\nappreciate the generality of the reinforcement learning framework. One of the most exciting aspects of modern reinforcement learning is its substantive\nand fruitful interactions with other engineering and scienti\ufb01c disciplines. Reinforcement\nlearning is part of a decades-long trend within arti\ufb01cial intelligence and machine learn-\ning toward greater integration with statistics, optimization, and other mathematical\nsubjects. For example, the ability of some reinforcement learning methods to learn with\nparameterized approximators addresses the classical \u201ccurse of dimensionality\u201d in oper-\nations research and control theory. More distinctively, reinforcement learning has also\ninteracted strongly with psychology and neuroscience, with substantial bene\ufb01ts going\nboth ways. Of all the forms of machine learning, reinforcement learning is the clos-\nest to the kind of learning that humans and other animals do, and many of the core\nalgorithms of reinforcement learning were originally inspired by biological learning sys-\ntems. Reinforcement learning has also given back, both through a psychological model\nof animal learning that better matches some of the empirical data, and through an in-\n\ufb02uential model of parts of the brain\u2019s reward system. The body of this book develops\nthe ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters 14 and 15.",
    "chapter_id": 137,
    "score": 0.6345391273498535,
    "page": 24
  },
  "7437": {
    "text": "sequences. Second, reinforcement learning algorithms are associative, meaning that\nthe alternatives found by selection are associated with particular situations, or states,\nto form the agent\u2019s policy. Like learning described by the Law of E\ufb00ect, reinforcement\nlearning is not just the process of \ufb01nding actions that produce a lot of reward, but also\nof connecting these actions to situations or states. Thorndike used the phrase learning\nby \u201cselecting and connecting\u201d (Hilgard, 1956). Natural selection in evolution is a prime\nexample of a selectional process, but it is not associative (at least as it is commonly\nunderstood); supervised learning is associative, but it is not selectional because it relies\non instructions that directly tell the agent how to change its behavior. In computational terms, the Law of E\ufb00ect describes an elementary way of combining\nsearch and memory: search in the form of trying and selecting among many actions\nin each situation, and memory in the form of associations linking situations with the\nactions found\u2014so far\u2014to work best in those situations. Search and memory are essential\ncomponents of all reinforcement learning algorithms, whether memory takes the form of\nan agent\u2019s policy, value function, or environment model. A reinforcement learning algorithm\u2019s need to search means that it has to explore in\nsome way. Animals clearly explore as well, and early animal learning researchers dis-\nagreed about the degree of guidance an animal uses in selecting its actions in situations\nlike Thorndike\u2019s puzzle boxes. Are actions the result of \u201cabsolutely random, blind grop-\ning\u201d (Woodworth, 1938, p. 777), or is there some degree of guidance, either from prior\nlearning, reasoning, or other means? Although some thinkers, including Thorndike, seem\nto have taken the former position, others favored more deliberate exploration. Reinforce-\nment learning algorithms allow wide latitude for how much guidance an agent can employ\nin selecting actions.",
    "chapter_id": 150,
    "score": 0.6201447248458862,
    "page": 382
  },
  "7649": {
    "text": "s \u201cimitation learning,\u201d \u201clearning from\ndemonstration,\u201d and \u201capprenticeship learning.\u201d The idea here is to bene\ufb01t from the ex-\npert agent but leave open the possibility of eventually performing better. Learning from\nan expert\u2019s behavior can be done either by learning directly by supervised learning or\nby extracting a reward signal using what is known as \u201cinverse reinforcement learning\u201d\nand then using a reinforcement learning algorithm with that reward signal to learn a\npolicy. The task of inverse reinforcement learning as explored by Ng and Russell (2000)\nis to try to recover the expert\u2019s reward signal from the expert\u2019s behavior alone. This\ncannot be done exactly because a policy can be optimal with respect to many di\ufb00er-\nent reward signals (for example, any reward signal that gives the same reward for all\nstates and actions), but it is possible to \ufb01nd plausible reward signal candidates. Un-\nfortunately, strong assumptions are required, including knowledge of the environment\u2019s\ndynamics and of the feature vectors in which the reward signal is linear. The method\nalso requires completely solving the problem (e.g., by dynamic programming methods)\nmultiple times. These di\ufb03culties notwithstanding, Abbeel and Ng (2004) argue that the\ninverse reinforcement learning approach can sometimes be more e\ufb00ective than supervised\nlearning for bene\ufb01ting from the behavior of an expert. Another approach to \ufb01nding a good reward signal is to automate the trial-and-error\nsearch for a good signal that we mentioned above. From an application perspective, the\nreward signal is a parameter of the learning algorithm. As is true for other algorithm\nparameters, the search for a good reward signal can be automated by de\ufb01ning a space of\nfeasible candidates and applying an optimization algorithm.",
    "chapter_id": 153,
    "score": 0.5914607048034668,
    "page": 494
  }
}