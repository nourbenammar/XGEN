**Instructions:**
You are an AI assistant. Answer the User Question accurately and concisely based *only* on the information available in the provided Context Chunks.

**Context Chunks:**

--- Context Source 1 ---
Bell-
man optimality equation (4.1) holds, and thus that the policy and the value function are
optimal. The evaluation and improvement processes in GPI can be viewed as both competing
and cooperating. They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically makes the value
function incorrect for the changed policy, and making the value function consistent with
the policy typically causes that policy no longer to be greedy. In the long run, however,

4.7. Eﬃciency of Dynamic Programming
87
these two processes interact to ﬁnd a single joint solution: the optimal value function
and an optimal policy. v⇤, ⇡⇤
⇡ = greedy(v)
v, ⇡
v = v⇡
One might also think of the interaction between
the evaluation and improvement processes in GPI
in terms of two constraints or goals—for example,
as two lines in two-dimensional space as suggested
by the diagram to the right. Although the real
geometry is much more complicated than this, the
diagram suggests what happens in the real case. Each process drives the value function or policy
toward one of the lines representing a solution to
one of the two goals. The goals interact because the two lines are not orthogonal. Driving
directly toward one goal causes some movement away from the other goal. Inevitably,
however, the joint process is brought closer to the overall goal of optimality. The arrows
in this diagram correspond to the behavior of policy iteration in that each takes the
system all the way to achieving one of the two goals completely. In GPI one could
also take smaller, incomplete steps toward each goal. In either case, the two processes
together achieve the overall goal of optimality even though neither is attempting to
achieve it directly. 4.7
Eﬃciency of Dynamic Programming
DP may not be practical for very large problems, but compared with other methods
for solving MDPs, DP methods are actually quite eﬃcient.

--- Context Source 2 ---
e state set (we explore this further in
Chapter 8). A fourth advantage of Monte Carlo methods, which we discuss later in the book, is
that they may be less harmed by violations of the Markov property. This is because
they do not update their value estimates on the basis of the value estimates of successor
states. In other words, it is because they do not bootstrap. In designing Monte Carlo control methods we have followed the overall schema of
generalized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting
processes of policy evaluation and policy improvement. Monte Carlo methods provide
an alternative policy evaluation process. Rather than use a model to compute the value
of each state, they simply average many returns that start in the state. Because a state’s
value is the expected return, this average can become a good approximation to the
value. In control methods we are particularly interested in approximating action-value
functions, because these can be used to improve the policy without requiring a model of
the environment’s transition dynamics. Monte Carlo methods intermix policy evaluation
and policy improvement steps on an episode-by-episode basis, and can be incrementally
implemented on an episode-by-episode basis. Maintaining suﬃcient exploration is an issue in Monte Carlo control methods. It is
not enough just to select the actions currently estimated to be best, because then no
returns will be obtained for alternative actions, and it may never be learned that they are
actually better. One approach is to ignore this problem by assuming that episodes begin
with state–action pairs randomly selected to cover all possibilities. Such exploring starts
can sometimes be arranged in applications with simulated episodes, but are unlikely
in learning from real experience. In on-policy methods, the agent commits to always
exploring and tries to ﬁnd the best policy that still explores.

--- Context Source 3 ---
mputers to solve MDPs with
millions of states. Both policy iteration and value iteration are widely used, and it is not

88
Chapter 4: Dynamic Programming
clear which, if either, is better in general. In practice, these methods usually converge
much faster than their theoretical worst-case run times, particularly if they are started
with good initial value functions or policies. On problems with large state spaces, asynchronous DP methods are often preferred. To complete even one sweep of a synchronous method requires computation and mem-
ory for every state. For some problems, even this much memory and computation is
impractical, yet the problem is still potentially solvable because relatively few states
occur along optimal solution trajectories. Asynchronous methods and other variations
of GPI can be applied in such cases and may ﬁnd good or optimal policies much faster
than synchronous methods can. 4.8
Summary
In this chapter we have become familiar with the basic ideas and algorithms of dynamic
programming as they relate to solving ﬁnite MDPs. Policy evaluation refers to the (typi-
cally) iterative computation of the value functions for a given policy. Policy improvement
refers to the computation of an improved policy given the value function for that policy. Putting these two computations together, we obtain policy iteration and value iteration,
the two most popular DP methods. Either of these can be used to reliably compute
optimal policies and value functions for ﬁnite MDPs given complete knowledge of the
MDP. Classical DP methods operate in sweeps through the state set, performing an expected
update operation on each state. Each such operation updates the value of one state based
on the values of all possible successor states and their probabilities of occurring. Ex-
pected updates are closely related to Bellman equations: they are little more than these
equations turned into assignment statements.

--- Context Source 4 ---
s. Also contributing may have been the preva-
lent view of dynamic programming as an oﬀ-line computation depending essentially on
accurate system models and analytic solutions to the Bellman equation. Further, the
simplest form of dynamic programming is a computation that proceeds backwards in
time, making it diﬃcult to see how it could be involved in a learning process that must
proceed in a forward direction. Some of the earliest work in dynamic programming,
such as that by Bellman and Dreyfus (1959), might now be classiﬁed as following a
learning approach. Witten’s (1977) work (discussed below) certainly qualiﬁes as a com-
bination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly

1.7. Early History of Reinforcement Learning
15
for greater interrelation of dynamic programming and learning methods and its rele-
vance to understanding neural and cognitive mechanisms. For us the full integration
of dynamic programming methods with on-line learning did not occur until the work
of Chris Watkins in 1989, whose treatment of reinforcement learning using the MDP
formalism has been widely adopted. Since then these relationships have been exten-
sively developed by many researchers, most particularly by Dimitri Bertsekas and John
Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer to the
combination of dynamic programming and neural networks. Another term currently in
use is “approximate dynamic programming.” These various approaches emphasize dif-
ferent aspects of the subject, but they all share with reinforcement learning an interest
in circumventing the classical shortcomings of dynamic programming. We would consider all of the work in optimal control also to be, in a sense, work
in reinforcement learning.

--- Context Source 5 ---
playing by the opponent. For example, a minimax player would never reach a game state from which it could
lose, even if in fact it always won from that state because of incorrect play by the
opponent. Classical optimization methods for sequential decision problems, such as
dynamic programming, can compute an optimal solution for any opponent, but require
as input a complete speciﬁcation of that opponent, including the probabilities with which
the opponent makes each move in each board state. Let us assume that this information

1.5. An Extended Example: Tic-Tac-Toe
9
is not available a priori for this problem, as it is not for the vast majority of problems of
practical interest. On the other hand, such information can be estimated from experience,
in this case by playing many games against the opponent. About the best one can do
on this problem is ﬁrst to learn a model of the opponent’s behavior, up to some level of
conﬁdence, and then apply dynamic programming to compute an optimal solution given
the approximate opponent model. In the end, this is not that diﬀerent from some of the
reinforcement learning methods we examine later in this book. An evolutionary method applied to this problem would directly search the space of
possible policies for one with a high probability of winning against the opponent. Here,
a policy is a rule that tells the player what move to make for every state of the game—
every possible conﬁguration of Xs and Os on the three-by-three board. For each policy
considered, an estimate of its winning probability would be obtained by playing some
number of games against the opponent. This evaluation would then direct which policy
or policies were considered next. A typical evolutionary method would hill-climb in
policy space, successively generating and evaluating policies in an attempt to obtain
incremental improvements. Or, perhaps, a genetic-style algorithm could be used that
would maintain and evaluate a population of policies.


**User Question:**

"What is policy evaluation in dynamic programming?"

---

**Answer:**
