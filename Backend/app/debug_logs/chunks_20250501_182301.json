{
  "6894": {
    "text": "body of this book develops\nthe ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters 14 and 15. Finally, reinforcement learning is also part of a larger trend in arti\ufb01cial intelligence\nback toward simple general principles. Since the late 1960\u2019s, many arti\ufb01cial intelligence\nresearchers presumed that there are no general principles to be discovered, that intelli-\ngence is instead due to the possession of a vast number of special purpose tricks, proce-\ndures, and heuristics. It was sometimes said that if we could just get enough relevant\nfacts into a machine, say one million, or one billion, then it would become intelligent. Methods based on general principles, such as search or learning, were characterized as\n\u201cweak methods,\u201d whereas those based on speci\ufb01c knowledge were called \u201cstrong meth-\nods.\u201d This view is still common today, but not dominant. From our point of view, it was\nsimply premature: too little e\ufb00ort had been put into the search for general principles to\nconclude that there were none. Modern arti\ufb01cial intelligence now includes much research\nlooking for general principles of learning, search, and decision making, as well as trying\nto incorporate vast amounts of domain knowledge. It is not clear how far back the pen-\ndulum will swing, but reinforcement learning research is certainly part of the swing back\ntoward simpler and fewer general principles of arti\ufb01cial intelligence. 1.2\nExamples\nA good way to understand reinforcement learning is to consider some of the examples\nand possible applications that have guided its development. \u2022 A master chess player makes a move. The choice is informed both by planning\u2014\nanticipating possible replies and counterreplies\u2014and by immediate, intuitive judg-\nments of the desirability of particular positions and moves. \u2022 An adaptive controller adjusts parameters of a petroleum re\ufb01nery\u2019s operation in\n\n1.2. Examples\n5\nreal time.",
    "chapter_id": 137,
    "score": 0.6250981688499451,
    "page": 24
  },
  "7461": {
    "text": "aspects of behavior studied by ethologists and behav-\nioral ecologists: how animals relate to one another and to their physical surroundings,\nand how their behavior contributes to evolutionary \ufb01tness. Optimization, MDPs, and\ndynamic programming \ufb01gure prominently in these \ufb01elds, and our emphasis on agent in-\nteraction with dynamic environments connects to the study of agent behavior in complex\n\u201cecologies.\u201d Multi-agent reinforcement learning, omitted in this book, has connections\nto social aspects of behavior. Despite the lack of treatment here, reinforcement learn-\ning should by no means be interpreted as dismissing evolutionary perspectives. Nothing\nabout reinforcement learning implies a tabula rasa view of learning and behavior. Indeed,\nexperience with engineering applications has highlighted the importance of building into\nreinforcement learning systems knowledge that is analogous to what evolution provides\nto animals. Bibliographical and Historical Remarks\nLudvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in\nthe contexts of psychology and neuroscience. These publications are useful companions\nto this chapter and the following chapter on reinforcement learning and neuroscience. 14.1\nDayan, Niv, Seymour, and Daw (2006) focused on interactions between clas-\nsical and instrumental conditioning, particularly situations where classically-\nconditioned and instrumental responses are in con\ufb02ict. They proposed a Q-\nlearning framework for modeling aspects of this interaction. Modayil and Sut-\nton (2014) used a mobile robot to demonstrate the e\ufb00ectiveness of a control\nmethod combining a \ufb01xed response with online prediction learning. Calling\nthis Pavlovian control, they emphasized that it di\ufb00ers from the usual control\nmethods of reinforcement learning, being based on predictively executing \ufb01xed\nresponses and not on reward maximization.",
    "chapter_id": 150,
    "score": 0.6086462140083313,
    "page": 394
  },
  "6891": {
    "text": "inforcement learning, but\nby itself does not address the reinforcement learning problem of maximizing a reward\nsignal. We therefore consider reinforcement learning to be a third machine learning\n\n1.1. Reinforcement Learning\n3\nparadigm, alongside supervised learning and unsupervised learning and perhaps other\nparadigms as well. One of the challenges that arise in reinforcement learning, and not in other kinds\nof learning, is the trade-o\ufb00 between exploration and exploitation. To obtain a lot of\nreward, a reinforcement learning agent must prefer actions that it has tried in the past\nand found to be e\ufb00ective in producing reward. But to discover such actions, it has to\ntry actions that it has not selected before. The agent has to exploit what it has already\nexperienced in order to obtain reward, but it also has to explore in order to make better\naction selections in the future. The dilemma is that neither exploration nor exploitation\ncan be pursued exclusively without failing at the task. The agent must try a variety\nof actions and progressively favor those that appear to be best. On a stochastic task,\neach action must be tried many times to gain a reliable estimate of its expected reward. The exploration\u2013exploitation dilemma has been intensively studied by mathematicians\nfor many decades, yet remains unresolved. For now, we simply note that the entire\nissue of balancing exploration and exploitation does not even arise in supervised and\nunsupervised learning, at least in their purest forms. Another key feature of reinforcement learning is that it explicitly considers the whole\nproblem of a goal-directed agent interacting with an uncertain environment. This is in\ncontrast to many approaches that consider subproblems without addressing how they\nmight \ufb01t into a larger picture. For example, we have mentioned that much of machine\nlearning research is concerned with supervised learning without explicitly specifying how\nsuch an ability would \ufb01nally be useful.",
    "chapter_id": 137,
    "score": 0.6006850004196167,
    "page": 23
  },
  "6912": {
    "text": " long and rich,\nthat were pursued independently before intertwining in modern reinforcement learning. One thread concerns learning by trial and error that started in the psychology of animal\nlearning. This thread runs through some of the earliest work in arti\ufb01cial intelligence\nand led to the revival of reinforcement learning in the early 1980s. The other thread\n\n14\nChapter 1: Introduction\nconcerns the problem of optimal control and its solution using value functions and dy-\nnamic programming. For the most part, this thread did not involve learning. Although\nthe two threads have been largely independent, the exceptions revolve around a third,\nless distinct thread concerning temporal-di\ufb00erence methods such as the one used in the\ntic-tac-toe example in this chapter. All three threads came together in the late 1980s to\nproduce the modern \ufb01eld of reinforcement learning as we present it in this book. The thread focusing on trial-and-error learning is the one with which we are most\nfamiliar and about which we have the most to say in this brief history. Before doing\nthat, however, we brie\ufb02y discuss the optimal control thread. The term \u201coptimal control\u201d came into use in the late 1950s to describe the problem of\ndesigning a controller to minimize a measure of a dynamical system\u2019s behavior over time. One of the approaches to this problem was developed in the mid-1950s by Richard Bell-\nman and others through extending a nineteenth century theory of Hamilton and Jacobi. This approach uses the concepts of a dynamical system\u2019s state and of a value function,\nor \u201coptimal return function,\u201d to de\ufb01ne a functional equation, now often called the Bell-\nman equation. The class of methods for solving optimal control problems by solving\nthis equation came to be known as dynamic programming (Bellman, 1957a).",
    "chapter_id": 137,
    "score": 0.5854856967926025,
    "page": 34
  },
  "7445": {
    "text": " by\nstimulus traces, Hull (1943) proposed that longer gradients result from conditioned re-\ninforcement passing backwards from the goal, a process acting in conjunction with his\nmolar stimulus traces. Animal experiments showed that if conditions favor the devel-\nopment of conditioned reinforcement during a delay period, learning does not decrease\nwith increased delay as much as it does under conditions that obstruct secondary rein-\nforcement. Conditioned reinforcement is favored if there are stimuli that regularly occur\nduring the delay interval. Then it is as if reward is not actually delayed because there\nis more immediate conditioned reinforcement. Hull therefore envisioned that there is a\nprimary gradient based on the delay of the primary reinforcement mediated by stimulus\ntraces, and that this is progressively modi\ufb01ed, and lengthened, by conditioned reinforce-\nment. Algorithms presented in this book that use both eligibility traces and value functions\nto enable learning with delayed reinforcement correspond to Hull\u2019s hypothesis about\nhow animals are able to learn under these conditions. The actor\u2013critic architecture\ndiscussed in Sections 13.5, 15.7, and 15.8 illustrates this correspondence most clearly. The critic uses a TD algorithm to learn a value function associated with the system\u2019s\ncurrent behavior, that is, to predict the current policy\u2019s return. The actor updates the\ncurrent policy based on the critic\u2019s predictions, or more exactly, on changes in the critic\u2019s\npredictions. The TD error produced by the critic acts as a conditioned reinforcement\nsignal for the actor, providing an immediate evaluation of performance even when the\nprimary reward signal itself is considerably delayed. Algorithms that estimate action-\nvalue functions, such as Q-learning and Sarsa, similarly use TD learning principles to\nenable learning with delayed reinforcement by means of conditioned reinforcement.",
    "chapter_id": 150,
    "score": 0.5846325755119324,
    "page": 386
  }
}