{
  "6899": {
    "text": "pose of estimating\nvalues is to achieve more reward. Nevertheless, it is values with which we are most\nconcerned when making and evaluating decisions. Action choices are made based on\nvalue judgments. We seek actions that bring about states of highest value, not high-\nest reward, because these actions obtain the greatest amount of reward for us over the\nlong run. Unfortunately, it is much harder to determine values than it is to determine\n\n1.4. Limitations and Scope\n7\nrewards. Rewards are basically given directly by the environment, but values must be\nestimated and re-estimated from the sequences of observations an agent makes over its\nentire lifetime. In fact, the most important component of almost all reinforcement learn-\ning algorithms we consider is a method for e\ufb03ciently estimating values. The central role\nof value estimation is arguably the most important thing that has been learned about\nreinforcement learning over the last six decades. The fourth and \ufb01nal element of some reinforcement learning systems is a model of the\nenvironment. This is something that mimics the behavior of the environment, or more\ngenerally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state\nand next reward. Models are used for planning, by which we mean any way of deciding\non a course of action by considering possible future situations before they are actually\nexperienced. Methods for solving reinforcement learning problems that use models and\nplanning are called model-based methods, as opposed to simpler model-free methods\nthat are explicitly trial-and-error learners\u2014viewed as almost the opposite of planning. In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial\nand error, learn a model of the environment, and use the model for planning.",
    "chapter_id": 137,
    "score": 0.6523870825767517,
    "page": 27
  },
  "6907": {
    "text": " policies,\nbut learning a value function takes advantage of information available during the course\nof play. This simple example illustrates some of the key features of reinforcement learning\nmethods. First, there is the emphasis on learning while interacting with an environment,\nin this case with an opponent player. Second, there is a clear goal, and correct behavior\nrequires planning or foresight that takes into account delayed e\ufb00ects of one\u2019s choices. For example, the simple reinforcement learning player would learn to set up multi-move\ntraps for a shortsighted opponent. It is a striking feature of the reinforcement learning\nsolution that it can achieve the e\ufb00ects of planning and lookahead without using a model\nof the opponent and without conducting an explicit search over possible sequences of\nfuture states and actions. While this example illustrates some of the key features of reinforcement learning, it is\nso simple that it might give the impression that reinforcement learning is more limited\nthan it really is. Although tic-tac-toe is a two-person game, reinforcement learning also\napplies in the case in which there is no external adversary, that is, in the case of a\n\u201cgame against nature.\u201d\nReinforcement learning also is not restricted to problems in\nwhich behavior breaks down into separate episodes, like the separate games of tic-tac-\ntoe, with reward only at the end of each episode. It is just as applicable when behavior\ncontinues inde\ufb01nitely and when rewards of various magnitudes can be received at any\ntime. Reinforcement learning is also applicable to problems that do not even break\ndown into discrete time steps, like the plays of tic-tac-toe. The general principles apply\nto continuous-time problems as well, although the theory gets more complicated and we\nomit it from this introductory treatment. Tic-tac-toe has a relatively small, \ufb01nite state set, whereas reinforcement learning can\nbe used when the state set is very large, or even in\ufb01nite.",
    "chapter_id": 137,
    "score": 0.621798038482666,
    "page": 31
  },
  "7459": {
    "text": "rning agents. There is also a connection to the idea of an animal\u2019s motivational state,\nwhich in\ufb02uences what an animal will approach or avoid and what events are rewarding\nor punishing for the animal. The reinforcement learning algorithms presented in this book include two basic mech-\nanisms for addressing the problem of delayed reinforcement: eligibility traces and value\nfunctions learned via TD algorithms. Both mechanisms have antecedents in theories of\nanimal learning. Eligibility traces are similar to stimulus traces of early theories, and\nvalue functions correspond to the role of secondary reinforcement in providing nearly\nimmediate evaluative feedback. The next correspondence the chapter addressed is that between reinforcement learn-\ning\u2019s environment models and what psychologists call cognitive maps. Experiments con-\nducted in the mid 20th century purported to demonstrate the ability of animals to learn\ncognitive maps as alternatives to, or as additions to, state\u2013action associations, and later\nuse them to guide behavior, especially when the environment changes unexpectedly. En-\nvironment models in reinforcement learning are like cognitive maps in that they can be\nlearned by supervised learning methods without relying on reward signals, and then they\ncan be used later to plan behavior. Reinforcement learning\u2019s distinction between model-free and model-based algorithms\ncorresponds to the distinction in psychology between habitual and goal-directed behavior. Model-free algorithms make decisions by accessing information that has been strored in\na policy or an action-value function, whereas model-based methods select actions as the\nresult of planning ahead using a model of the agent\u2019s environment. Outcome-devaluation\nexperiments provide information about whether an animal\u2019s behavior is habitual or under\ngoal-directed control. Reinforcement learning theory has helped clarify thinking about\nthese issues.",
    "chapter_id": 150,
    "score": 0.6204330325126648,
    "page": 393
  },
  "6999": {
    "text": " agent has a complete and accurate environment model, the agent is typi-\ncally unable to perform enough computation per time step to fully use it. The memory\navailable is also an important constraint. Memory may be required to build up accurate\napproximations of value functions, policies, and models. In most cases of practical inter-\nest there are far more states than could possibly be entries in a table, and approximations\nmust be made. A well-de\ufb01ned notion of optimality organizes the approach to learning we describe in\nthis book and provides a way to understand the theoretical properties of various learning\nalgorithms, but it is an ideal that reinforcement learning agents can only approximate\nto varying degrees. In reinforcement learning we are very much concerned with cases in\nwhich optimal solutions cannot be found but must be approximated in some way. Bibliographical and Historical Remarks\nThe reinforcement learning problem is deeply indebted to the idea of Markov decision\nprocesses (MDPs) from the \ufb01eld of optimal control. These historical in\ufb02uences and other\nmajor in\ufb02uences from psychology are described in the brief history given in Chapter 1. Reinforcement learning adds to MDPs a focus on approximation and incomplete infor-\nmation for realistically large problems. MDPs and the reinforcement learning problem\nare only weakly linked to traditional learning and decision-making problems in arti\ufb01cial\nintelligence. However, arti\ufb01cial intelligence is now vigorously exploring MDP formula-\ntions for planning and decision making from a variety of perspectives. MDPs are more\ngeneral than previous formulations used in arti\ufb01cial intelligence in that they permit more\ngeneral kinds of goals and uncertainty. The theory of MDPs is treated by, e.g., Bertsekas (2005), White (1969), Whittle\n(1982, 1983), and Puterman (1994). A particularly compact treatment of the \ufb01nite case\nis given by Ross (1983).",
    "chapter_id": 139,
    "score": 0.6030058860778809,
    "page": 90
  },
  "7461": {
    "text": "aspects of behavior studied by ethologists and behav-\nioral ecologists: how animals relate to one another and to their physical surroundings,\nand how their behavior contributes to evolutionary \ufb01tness. Optimization, MDPs, and\ndynamic programming \ufb01gure prominently in these \ufb01elds, and our emphasis on agent in-\nteraction with dynamic environments connects to the study of agent behavior in complex\n\u201cecologies.\u201d Multi-agent reinforcement learning, omitted in this book, has connections\nto social aspects of behavior. Despite the lack of treatment here, reinforcement learn-\ning should by no means be interpreted as dismissing evolutionary perspectives. Nothing\nabout reinforcement learning implies a tabula rasa view of learning and behavior. Indeed,\nexperience with engineering applications has highlighted the importance of building into\nreinforcement learning systems knowledge that is analogous to what evolution provides\nto animals. Bibliographical and Historical Remarks\nLudvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in\nthe contexts of psychology and neuroscience. These publications are useful companions\nto this chapter and the following chapter on reinforcement learning and neuroscience. 14.1\nDayan, Niv, Seymour, and Daw (2006) focused on interactions between clas-\nsical and instrumental conditioning, particularly situations where classically-\nconditioned and instrumental responses are in con\ufb02ict. They proposed a Q-\nlearning framework for modeling aspects of this interaction. Modayil and Sut-\nton (2014) used a mobile robot to demonstrate the e\ufb00ectiveness of a control\nmethod combining a \ufb01xed response with online prediction learning. Calling\nthis Pavlovian control, they emphasized that it di\ufb00ers from the usual control\nmethods of reinforcement learning, being based on predictively executing \ufb01xed\nresponses and not on reward maximization.",
    "chapter_id": 150,
    "score": 0.5994601845741272,
    "page": 394
  }
}