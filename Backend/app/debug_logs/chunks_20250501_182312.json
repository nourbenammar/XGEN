{
  "6889": {
    "text": "2\nChapter 1: Introduction\nreward but also the next situation and, through that, all subsequent rewards. These two\ncharacteristics\u2014trial-and-error search and delayed reward\u2014are the two most important\ndistinguishing features of reinforcement learning. Reinforcement learning, like many topics whose names end with \u201cing,\u201d such as machine\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\nthat work well on the problem, and the \ufb01eld that studies this problem and its solution\nmethods. It is convenient to use a single name for all three things, but at the same time\nessential to keep the three conceptually separate. In particular, the distinction between\nproblems and solution methods is very important in reinforcement learning; failing to\nmake this distinction is the source of many confusions. We formalize the problem of reinforcement learning using ideas from dynamical sys-\ntems theory, speci\ufb01cally, as the optimal control of incompletely-known Markov decision\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\nis simply to capture the most important aspects of the real problem facing a learning\nagent interacting over time with its environment to achieve a goal. A learning agent\nmust be able to sense the state of its environment to some extent and must be able to\ntake actions that a\ufb00ect the state. The agent also must have a goal or goals relating to\nthe state of the environment. Markov decision processes are intended to include just\nthese three aspects\u2014sensation, action, and goal\u2014in their simplest possible forms with-\nout trivializing any of them. Any method that is well suited to solving such problems\nwe consider to be a reinforcement learning method. Reinforcement learning is di\ufb00erent from supervised learning, the kind of learning stud-\nied in most current research in the \ufb01eld of machine learning.",
    "chapter_id": 137,
    "score": 0.6603752970695496,
    "page": 22
  },
  "6893": {
    "text": " agent can also be a component of a larger behaving system. In this case,\nthe agent directly interacts with the rest of the larger system and indirectly interacts\nwith the larger system\u2019s environment. A simple example is an agent that monitors the\ncharge level of robot\u2019s battery and sends commands to the robot\u2019s control architecture. 4\nChapter 1: Introduction\nThis agent\u2019s environment is the rest of the robot together with the robot\u2019s environment. One must look beyond the most obvious examples of agents and their environments to\nappreciate the generality of the reinforcement learning framework. One of the most exciting aspects of modern reinforcement learning is its substantive\nand fruitful interactions with other engineering and scienti\ufb01c disciplines. Reinforcement\nlearning is part of a decades-long trend within arti\ufb01cial intelligence and machine learn-\ning toward greater integration with statistics, optimization, and other mathematical\nsubjects. For example, the ability of some reinforcement learning methods to learn with\nparameterized approximators addresses the classical \u201ccurse of dimensionality\u201d in oper-\nations research and control theory. More distinctively, reinforcement learning has also\ninteracted strongly with psychology and neuroscience, with substantial bene\ufb01ts going\nboth ways. Of all the forms of machine learning, reinforcement learning is the clos-\nest to the kind of learning that humans and other animals do, and many of the core\nalgorithms of reinforcement learning were originally inspired by biological learning sys-\ntems. Reinforcement learning has also given back, both through a psychological model\nof animal learning that better matches some of the empirical data, and through an in-\n\ufb02uential model of parts of the brain\u2019s reward system. The body of this book develops\nthe ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence,\nwith connections to psychology and neuroscience summarized in Chapters 14 and 15.",
    "chapter_id": 137,
    "score": 0.6515374183654785,
    "page": 24
  },
  "7459": {
    "text": "rning agents. There is also a connection to the idea of an animal\u2019s motivational state,\nwhich in\ufb02uences what an animal will approach or avoid and what events are rewarding\nor punishing for the animal. The reinforcement learning algorithms presented in this book include two basic mech-\nanisms for addressing the problem of delayed reinforcement: eligibility traces and value\nfunctions learned via TD algorithms. Both mechanisms have antecedents in theories of\nanimal learning. Eligibility traces are similar to stimulus traces of early theories, and\nvalue functions correspond to the role of secondary reinforcement in providing nearly\nimmediate evaluative feedback. The next correspondence the chapter addressed is that between reinforcement learn-\ning\u2019s environment models and what psychologists call cognitive maps. Experiments con-\nducted in the mid 20th century purported to demonstrate the ability of animals to learn\ncognitive maps as alternatives to, or as additions to, state\u2013action associations, and later\nuse them to guide behavior, especially when the environment changes unexpectedly. En-\nvironment models in reinforcement learning are like cognitive maps in that they can be\nlearned by supervised learning methods without relying on reward signals, and then they\ncan be used later to plan behavior. Reinforcement learning\u2019s distinction between model-free and model-based algorithms\ncorresponds to the distinction in psychology between habitual and goal-directed behavior. Model-free algorithms make decisions by accessing information that has been strored in\na policy or an action-value function, whereas model-based methods select actions as the\nresult of planning ahead using a model of the agent\u2019s environment. Outcome-devaluation\nexperiments provide information about whether an animal\u2019s behavior is habitual or under\ngoal-directed control. Reinforcement learning theory has helped clarify thinking about\nthese issues.",
    "chapter_id": 150,
    "score": 0.6388736963272095,
    "page": 393
  },
  "7542": {
    "text": "with further research. The new \ufb01eld of computational psychiatry similarly focuses on the use of computational\nmodels, some derived from reinforcement learning, to better understand mental disor-\nders. This chapter only touched the surface of how the neuroscience of reinforcement learn-\ning and the development of reinforcement learning in computer science and engineering\nhave in\ufb02uenced one another. Most features of reinforcement learning algorithms owe\ntheir design to purely computational considerations, but some have been in\ufb02uenced by\nhypotheses about neural learning mechanisms. Remarkably, as experimental data has\naccumulated about the brain\u2019s reward processes, many of the purely computationally-\nmotivated features of reinforcement learning algorithms are turning out to be consistent\nwith neuroscience data. Other features of computational reinforcement learning, such\neligibility traces and the ability of teams of reinforcement learning agents to learn to act\ncollectively under the in\ufb02uence of a globally-broadcast reinforcement signal, may also\nturn out to parallel experimental data as neuroscientists continue to unravel the neural\nbasis of reward-based animal learning and behavior. 15.13. Summary\n417\nBibliographical and Historical Remarks\nThe number of publications treating parallels between the neuroscience of learning and\ndecision making and the approach to reinforcement learning presented in this book is\nenormous. We can cite only a small selection. Niv (2009), Dayan and Niv (2008),\nGimcher (2011), Ludvig, Bellemare, and Pearson (2011), and Shah (2012) are good\nplaces to start. Together with economics, evolutionary biology, and mathematical psychology, rein-\nforcement learning theory is helping to formulate quantitative models of the neural mech-\nanisms of choice in humans and non-human primates. With its focus on learning, this\nchapter only lightly touches upon the neuroscience of decision making.",
    "chapter_id": 151,
    "score": 0.6381497383117676,
    "page": 437
  },
  "7405": {
    "text": "346\nChapter 14: Psychology\nof a role in psychology than they once did. But this experimentation led to the discovery\nof learning principles that are elemental and widespread throughout the animal king-\ndom, principles that should not be neglected in designing arti\ufb01cial learning systems. In\naddition, as we shall see, some aspects of cognitive processing connect naturally to the\ncomputational perspective provided by reinforcement learning. This chapter\u2019s \ufb01nal section includes references relevant to the connections we discuss\nas well as to connections we neglect. We hope this chapter encourages readers to probe all\nof these connections more deeply. Also included in this \ufb01nal section is a discussion of how\nthe terminology used in reinforcement learning relates to that of psychology. Many of\nthe terms and phrases used in reinforcement learning are borrowed from animal learning\ntheories, but the computational/engineering meanings of these terms and phrases do not\nalways coincide with their meanings in psychology. 14.1\nPrediction and Control\nThe algorithms we describe in this book fall into two broad categories: algorithms for pre-\ndiction and algorithms for control. These categories arise naturally in solution methods\nfor the reinforcement learning problem presented in Chapter 3. In many ways these cat-\negories respectively correspond to categories of learning extensively studied by psychol-\nogists: classical, or Pavlovian, conditioning and instrumental, or operant, conditioning. These correspondences are not completely accidental because of psychology\u2019s in\ufb02uence\non reinforcement learning, but they are nevertheless striking because they connect ideas\narising from di\ufb00erent objectives. The prediction algorithms presented in this book estimate quantities that depend\non how features of an agent\u2019s environment are expected to unfold over the future.",
    "chapter_id": 150,
    "score": 0.6240893602371216,
    "page": 366
  }
}