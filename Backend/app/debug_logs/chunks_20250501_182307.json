{
  "7030": {
    "text": "92\nChapter 5: Monte Carlo Methods\nTo handle the nonstationarity, we adapt the idea of general policy iteration (GPI)\ndeveloped in Chapter 4 for DP. Whereas there we computed value functions from knowl-\nedge of the MDP, here we learn value functions from sample returns with the MDP. The\nvalue functions and corresponding policies still interact to attain optimality in essentially\nthe same way (GPI). As in the DP chapter, \ufb01rst we consider the prediction problem (the\ncomputation of v\u03c0 and q\u03c0 for a \ufb01xed arbitrary policy \u03c0) then policy improvement, and,\n\ufb01nally, the control problem and its solution by GPI. Each of these ideas taken from DP\nis extended to the Monte Carlo case in which only sample experience is available. 5.1\nMonte Carlo Prediction\nWe begin by considering Monte Carlo methods for learning the state-value function for a\ngiven policy. Recall that the value of a state is the expected return\u2014expected cumulative\nfuture discounted reward\u2014starting from that state. An obvious way to estimate it from\nexperience, then, is simply to average the returns observed after visits to that state. As\nmore returns are observed, the average should converge to the expected value. This idea\nunderlies all Monte Carlo methods. In particular, suppose we wish to estimate v\u03c0(s), the value of a state s under policy \u03c0,\ngiven a set of episodes obtained by following \u03c0 and passing through s. Each occurrence\nof state s in an episode is called a visit to s. Of course, s may be visited multiple times\nin the same episode; let us call the \ufb01rst time it is visited in an episode the \ufb01rst visit\nto s. The \ufb01rst-visit MC method estimates v\u03c0(s) as the average of the returns following\n\ufb01rst visits to s, whereas the every-visit MC method averages the returns following all\nvisits to s. These two Monte Carlo (MC) methods are very similar but have slightly\ndi\ufb00erent theoretical properties. First-visit MC has been most widely studied, dating\nback to the 1940s, and is the one we focus on in this chapter.",
    "chapter_id": 141,
    "score": 0.5793088674545288,
    "page": 112
  },
  "7071": {
    "text": "s\nDP methods use an estimate of (6.4) as a target. The Monte Carlo target is an estimate\nbecause the expected value in (6.3) is not known; a sample return is used in place of\nthe real expected return. The DP target is an estimate not because of the expected\nvalues, which are assumed to be completely provided by a model of the environment,\nbut because v\u03c0(St+1) is not known and the current estimate, V (St+1), is used instead. The TD target is an estimate for both reasons: it samples the expected values in (6.4)\nand it uses the current estimate V instead of the true v\u03c0. Thus, TD methods combine\n\n6.1. TD Prediction\n121\nthe sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care\nand imagination this can take us a long way toward obtaining the advantages of both\nMonte Carlo and DP methods. TD(0)\nShown to the right is the backup diagram for tabular TD(0). The value\nestimate for the state node at the top of the backup diagram is updated on\nthe basis of the one sample transition from it to the immediately following\nstate. We refer to TD and Monte Carlo updates as sample updates because\nthey involve looking ahead to a sample successor state (or state\u2013action pair),\nusing the value of the successor and the reward along the way to compute a\nbacked-up value, and then updating the value of the original state (or state\u2013\naction pair) accordingly. Sample updates di\ufb00er from the expected updates\nof DP methods in that they are based on a single sample successor rather than on a\ncomplete distribution of all possible successors. Finally, note that the quantity in brackets in the TD(0) update is a sort of error,\nmeasuring the di\ufb00erence between the estimated value of St and the better estimate\nRt+1 +\u03b3V (St+1). This quantity, called the TD error, arises in various forms throughout\nreinforcement learning:\n\u03b4t .= Rt+1 + \u03b3V (St+1) \u2212 V (St). (6.5)\nNotice that the TD error at each time is the error in the estimate made at that time.",
    "chapter_id": 142,
    "score": 0.578964352607727,
    "page": 140
  },
  "7066": {
    "text": "e state set (we explore this further in\nChapter 8). A fourth advantage of Monte Carlo methods, which we discuss later in the book, is\nthat they may be less harmed by violations of the Markov property. This is because\nthey do not update their value estimates on the basis of the value estimates of successor\nstates. In other words, it is because they do not bootstrap. In designing Monte Carlo control methods we have followed the overall schema of\ngeneralized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting\nprocesses of policy evaluation and policy improvement. Monte Carlo methods provide\nan alternative policy evaluation process. Rather than use a model to compute the value\nof each state, they simply average many returns that start in the state. Because a state\u2019s\nvalue is the expected return, this average can become a good approximation to the\nvalue. In control methods we are particularly interested in approximating action-value\nfunctions, because these can be used to improve the policy without requiring a model of\nthe environment\u2019s transition dynamics. Monte Carlo methods intermix policy evaluation\nand policy improvement steps on an episode-by-episode basis, and can be incrementally\nimplemented on an episode-by-episode basis. Maintaining su\ufb03cient exploration is an issue in Monte Carlo control methods. It is\nnot enough just to select the actions currently estimated to be best, because then no\nreturns will be obtained for alternative actions, and it may never be learned that they are\nactually better. One approach is to ignore this problem by assuming that episodes begin\nwith state\u2013action pairs randomly selected to cover all possibilities. Such exploring starts\ncan sometimes be arranged in applications with simulated episodes, but are unlikely\nin learning from real experience. In on-policy methods, the agent commits to always\nexploring and tries to \ufb01nd the best policy that still explores.",
    "chapter_id": 141,
    "score": 0.514039933681488,
    "page": 136
  },
  "6931": {
    "text": "problems. The \ufb01rst chapter of this part of the book describes solution methods for the special\ncase of the reinforcement learning problem in which there is only a single state, called\nbandit problems. The second chapter describes the general problem formulation that we\ntreat throughout the rest of the book\u2014\ufb01nite Markov decision processes\u2014and its main\nideas including Bellman equations and value functions. The next three chapters describe three fundamental classes of methods for solving \ufb01nite\nMarkov decision problems: dynamic programming, Monte Carlo methods, and temporal-\ndi\ufb00erence learning. Each class of methods has its strengths and weaknesses. Dynamic\nprogramming methods are well developed mathematically, but require a complete and\naccurate model of the environment. Monte Carlo methods don\u2019t require a model and are\nconceptually simple, but are not well suited for step-by-step incremental computation. Finally, temporal-di\ufb00erence methods require no model and are fully incremental, but are\nmore complex to analyze. The methods also di\ufb00er in several ways with respect to their\ne\ufb03ciency and speed of convergence. The remaining two chapters describe how these three classes of methods can be com-\nbined to obtain the best features of each of them. In one chapter we describe how the\nstrengths of Monte Carlo methods can be combined with the strengths of temporal-\ndi\ufb00erence methods via multi-step bootstrapping methods. In the \ufb01nal chapter of this\npart of the book we show how temporal-di\ufb00erence learning methods can be combined\nwith model learning and planning methods (such as dynamic programming) for a com-\nplete and uni\ufb01ed solution to the tabular reinforcement learning problem. 23",
    "chapter_id": 137,
    "score": 0.46992939710617065,
    "page": 44
  },
  "7035": {
    "text": " These di\ufb00erences in the diagrams accurately re\ufb02ect the fundamental\ndi\ufb00erences between the algorithms. An important fact about Monte Carlo methods is that the estimates for each\nstate are independent. The estimate for one state does not build upon the estimate\nof any other state, as is the case in DP. In other words, Monte Carlo methods do\nnot bootstrap as we de\ufb01ned it in the previous chapter. In particular, note that the computational expense of estimating the value of\na single state is independent of the number of states. This can make Monte\nCarlo methods particularly attractive when one requires the value of only one\nor a subset of states. One can generate many sample episodes starting from the\nstates of interest, averaging returns from only these states, ignoring all others. This is\na third advantage Monte Carlo methods can have over DP methods (after the ability to\nlearn from actual experience and from simulated experience). A bubble on a wire loop. From Hersh and Griego (1969). Reproduced\nwith permission. Copyright (1969) Scienti\ufb01c\nAmerican, a division of Nature America, Inc.\nAll rights reserved. Example 5.2: Soap Bubble\nSuppose a wire frame forming a closed loop is\ndunked in soapy water to form a soap surface\nor bubble conforming at its edges to the wire\nframe. If the geometry of the wire frame is ir-\nregular but known, how can you compute the\nshape of the surface? The shape has the prop-\nerty that the total force on each point exerted\nby neighboring points is zero (or else the shape\nwould change). This means that the surface\u2019s\nheight at any point is the average of its heights\nat points in a small circle around that point. In\naddition, the surface must meet at its bound-\naries with the wire frame. The usual approach\nto problems of this kind is to put a grid over\nthe area covered by the surface and solve for its\nheight at the grid points by an iterative com-\nputation.",
    "chapter_id": 141,
    "score": 0.4668276011943817,
    "page": 115
  }
}