{
  "7028": {
    "text": "961) in commenting on Samuel\u2019s checkers player. In\na footnote, Minsky mentioned that it is possible to apply DP to problems in which\nSamuel\u2019s backing-up process can be handled in closed analytic form. This remark may\nhave misled arti\ufb01cial intelligence researchers into believing that DP was restricted to\nanalytically tractable problems and therefore largely irrelevant to arti\ufb01cial intelligence. Andreae (1969b) mentioned DP in the context of reinforcement learning, speci\ufb01cally\npolicy iteration, although he did not make speci\ufb01c connections between DP and learning\nalgorithms. Werbos (1977) suggested an approach to approximating DP called \u201cheuristic\ndynamic programming\u201d that emphasizes gradient-descent methods for continuous-state\nproblems (Werbos, 1982, 1987, 1988, 1989, 1992). These methods are closely related to\nthe reinforcement learning algorithms that we discuss in this book. Watkins (1989) was\nexplicit in connecting reinforcement learning to DP, characterizing a class of reinforce-\nment learning methods as \u201cincremental dynamic programming.\u201d\n4.1\u20134\nThese sections describe well-established DP algorithms that are covered in any\nof the general DP references cited above. The policy improvement theorem and\nthe policy iteration algorithm are due to Bellman (1957a) and Howard (1960). Our presentation was in\ufb02uenced by the local view of policy improvement taken\nby Watkins (1989). Our discussion of value iteration as a form of truncated\npolicy iteration is based on the approach of Puterman and Shin (1978), who\npresented a class of algorithms called modi\ufb01ed policy iteration, which includes\npolicy iteration and value iteration as special cases. An analysis showing how\n\n90\nChapter 4: Dynamic Programming\nvalue iteration can be made to \ufb01nd an optimal policy in \ufb01nite time is given by\nBertsekas (1987). Iterative policy evaluation is an example of a classical successive approximation\nalgorithm for solving a system of linear equations.",
    "chapter_id": 140,
    "score": 0.5868358612060547,
    "page": 109
  },
  "6913": {
    "text": "nctional equation, now often called the Bell-\nman equation. The class of methods for solving optimal control problems by solving\nthis equation came to be known as dynamic programming (Bellman, 1957a). Bellman\n(1957b) also introduced the discrete stochastic version of the optimal control problem\nknown as Markov decision processes (MDPs), and Ronald Howard (1960) devised the\npolicy iteration method for MDPs. All of these are essential elements underlying the\ntheory and algorithms of modern reinforcement learning. Dynamic programming is widely considered the only feasible way of solving general\nstochastic optimal control problems. It su\ufb00ers from what Bellman called \u201cthe curse of\ndimensionality,\u201d meaning that its computational requirements grow exponentially with\nthe number of state variables, but it is still far more e\ufb03cient and more widely applicable\nthan any other general method. Dynamic programming has been extensively developed\nsince the late 1950s, including extensions to partially observable MDPs (surveyed by\nLovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approxima-\ntion methods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982,\n1983). Many excellent modern treatments of dynamic programming are available (e.g.,\nBertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; and Whittle, 1982, 1983). Bryson\n(1996) provides an authoritative history of optimal control. Connections between optimal control and dynamic programming, on the one hand,\nand learning, on the other, were slow to be recognized. We cannot be sure about what\naccounted for this separation, but its main cause was likely the separation between the\ndisciplines involved and their di\ufb00erent goals. Also contributing may have been the preva-\nlent view of dynamic programming as an o\ufb00-line computation depending essentially on\naccurate system models and analytic solutions to the Bellman equation.",
    "chapter_id": 137,
    "score": 0.5665391683578491,
    "page": 34
  },
  "7022": {
    "text": "y greedy with respect to the current value function (policy improvement). In policy\niteration, these two processes alternate, each completing before the other begins, but\nthis is not really necessary. In value iteration, for example, only a single iteration of\npolicy evaluation is performed in between each policy improvement. In asynchronous\nDP methods, the evaluation and improvement processes are interleaved at an even \ufb01ner\ngrain. In some cases a single state is updated in one process before returning to the\nother. As long as both processes continue to update all states, the ultimate result is\ntypically the same\u2014convergence to the optimal value function and an optimal policy. evaluation\nimprovement\n\u21e1  greedy(V )\nV\n\u21e1\nV  v\u21e1\nv\u21e4\n\u21e1\u21e4\nWe use the term generalized policy iteration (GPI) to refer\nto the general idea of letting policy evaluation and policy im-\nprovement processes interact, independent of the granularity and\nother details of the two processes. Almost all reinforcement\nlearning methods are well described as GPI. That is, all have\nidenti\ufb01able policies and value functions, with the policy always\nbeing improved with respect to the value function and the value\nfunction always being driven toward the value function for the\npolicy, as suggested by the diagram to the right. It is easy to\nsee that if both the evaluation process and the improvement\nprocess stabilize, that is, no longer produce changes, then the\nvalue function and policy must be optimal. The value function\nstabilizes only when it is consistent with the current policy, and\nthe policy stabilizes only when it is greedy with respect to the\ncurrent value function. Thus, both processes stabilize only when a policy has been found\nthat is greedy with respect to its own evaluation function. This implies that the Bell-\nman optimality equation (4.1) holds, and thus that the policy and the value function are\noptimal. The evaluation and improvement processes in GPI can be viewed as both competing\nand cooperating.",
    "chapter_id": 140,
    "score": 0.5482499599456787,
    "page": 105
  },
  "7099": {
    "text": "apply widely. For example, afterstate methods are still aptly described in terms\nof generalized policy iteration, with a policy and (afterstate) value function interacting\nin essentially the same way. In many cases one will still face the choice between on-policy\nand o\ufb00-policy methods for managing the need for persistent exploration. Exercise 6.14\nDescribe how the task of Jack\u2019s Car Rental (Example 4.2) could be\nreformulated in terms of afterstates. Why, in terms of this speci\ufb01c task, would such a\nreformulation be likely to speed convergence? \u25a1\n6.9\nSummary\nIn this chapter we introduced a new kind of learning method, temporal-di\ufb00erence (TD)\nlearning, and showed how it can be applied to the reinforcement learning problem. As\nusual, we divided the overall problem into a prediction problem and a control problem. TD methods are alternatives to Monte Carlo methods for solving the prediction problem. In both cases, the extension to the control problem is via the idea of generalized policy\niteration (GPI) that we abstracted from dynamic programming. This is the idea that\n\n138\nChapter 6: Temporal-Di\ufb00erence Learning\napproximate policy and value functions should interact in such a way that they both\nmove toward their optimal values. One of the two processes making up GPI drives the value function to accurately\npredict returns for the current policy; this is the prediction problem. The other process\ndrives the policy to improve locally (e.g., to be \u03b5-greedy) with respect to the current value\nfunction. When the \ufb01rst process is based on experience, a complication arises concerning\nmaintaining su\ufb03cient exploration. We can classify TD control methods according to\nwhether they deal with this complication by using an on-policy or o\ufb00-policy approach. Sarsa is an on-policy method, and Q-learning is an o\ufb00-policy method. Expected Sarsa\nis also an o\ufb00-policy method as we present it here.",
    "chapter_id": 142,
    "score": 0.5458340048789978,
    "page": 157
  },
  "7618": {
    "text": "e of policy can result in more clicks by a\nuser over repeated visits to the site, and if the policy is suitably designed, more eventual\nsales. Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments\nto see if policies designed to maximize clicks over the long term could in fact improve over\nshort-term greedy policies. The Adobe Marketing Cloud, a set of tools that many com-\npanies use to run digital marketing campaigns, provides infrastructure for automating\nuser-targed advertising and fund-raising campaigns. Actually deploying novel policies\nusing these tools entails signi\ufb01cant risk because a new policy may end up performing\npoorly. For this reason, the research team needed to assess what a policy\u2019s performance\nwould be if it were to be actually deployed, but to do so on the basis of data collected\nunder the execution of other policies. A critical aspect of this research, then, was o\ufb00-\npolicy evaluation. Further, the team wanted to do this with high con\ufb01dence to reduce\nthe risk of deploying a new policy. Although high con\ufb01dence o\ufb00-policy evaluation was a\ncentral component of this research (see also Thomas, 2015; Thomas, Theocharous, and\nGhavamzadeh, 2015), here we focus only on the algorithms and their results. Theocharous et al. compared the results of two algorithms for learning ad recommen-\ndation policies. The \ufb01rst algorithm, which they called greedy optimization, had the goal\n\n456\nChapter 16: Applications and Case Studies\nof maximizing only the probability of immediate clicks. As in the standard contextual\nbandit formulation, this algorithm did not take the long-term e\ufb00ects of recommendations\ninto account. The other algorithm, a reinforcement learning algorithm based on an MDP\nformulation, aimed at improving the number of clicks users made over multiple visits to\na website. They called this latter algorithm life-time value (LTV) optimization.",
    "chapter_id": 152,
    "score": 0.5041608810424805,
    "page": 477
  }
}