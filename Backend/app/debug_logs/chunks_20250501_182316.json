{
  "7591": {
    "text": "e semi-gradient form of Q-learning. TD-Gammon estimated the values of\n\n442\nChapter 16: Applications and Case Studies\nafterstates, which were easily obtained from the rules for making backgammon moves. To use the same algorithm for the Atari games would have required generating the next\nstates for each possible action (which would not have been afterstates in that case). This could have been done by using the game emulator to run single-step simulations\nfor all the possible actions (which ALE makes possible). Or a model of each game\u2019s\nstate-transition function could have been learned and used to predict next states (Oh,\nGuo, Lee, Lewis, and Singh, 2015). While these methods might have produced results\ncomparable to DQN\u2019s, they would have been more complicated to implement and would\nhave signi\ufb01cantly increased the time needed for learning. Another motivation for using\nQ-learning was that DQN used the experience replay method, described below, which\nrequires an o\ufb00-policy algorithm. Being model-free and o\ufb00-policy made Q-learning a\nnatural choice. Before describing the details of DQN and how the experiments were conducted, we look\nat the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with\nthe scores of the best performing learning system in the literature at the time, the scores\nof a professional human games tester, and the scores of an agent that selected actions\nat random. The best system from the literature used linear function approximation\nwith features hand designed using some knowledge about Atari 2600 games (Bellemare,\nNaddaf, Veness, and Bowling, 2013). DQN learned on each game by interacting with the\ngame emulator for 50 million frames, which corresponds to about 38 days of experience\nwith the game. At the start of learning on each game, the weights of DQN\u2019s network\nwere reset to random values.",
    "chapter_id": 152,
    "score": 0.6048208475112915,
    "page": 463
  },
  "7596": {
    "text": "experience replay \ufb01rst studied by Lin (1992). This method stores the\nagent\u2019s experience at each time step in a replay memory that is accessed to perform the\nweight updates. It worked like this in DQN. After the game emulator executed action\nAt in a state represented by the image stack St, and returned reward Rt+1 and image\nstack St+1, it added the tuple (St, At, Rt+1, St+1) to the replay memory. This memory\naccumulated experiences over many plays of the same game. At each time step multiple\nQ-learning updates\u2014a mini-batch\u2014were performed based on experiences sampled uni-\nformly at random from the replay memory. Instead of St+1 becoming the new St for\nthe next update as it would in the usual form of Q-learning, a new unconnected expe-\nrience was drawn from the replay memory to supply data for the next update. because\nQ-learning is an o\ufb00-policy algorithm, it does not need to be applied along connected\ntrajectories. Q-learning with experience replay provided several advantages over the usual form of\nQ-learning. The ability to use each stored experience for many updates allowed DQN to\nlearn more e\ufb03ciently from its experiences. Experience replay reduced the variance of the\nupdates because successive updates were not correlated with one another as they would\nbe with standard Q-learning. And by removing the dependence of successive experiences\non the current weights, experience replay eliminated one source of instability. Mnih et al. modi\ufb01ed standard Q-learning in a second way to improve its stability. As in other methods that bootstrap, the target for a Q-learning update depends on the\ncurrent action-value function estimate. When a parameterized function approximation\nmethod is used to represent action values, the target is a function of the same param-\neters that are being updated. For example, the target in the update given by (16.3) is\n\u03b3 maxa \u02c6q(St+1, a, wt).",
    "chapter_id": 152,
    "score": 0.5500144362449646,
    "page": 466
  },
  "7294": {
    "text": " 1;\nsee Chapter 12) but often bootstrapping greatly increases e\ufb03ciency. It is an ability that\nwe would very much like to keep in our toolkit. Finally, there is o\ufb00-policy learning; can we give that up? On-policy methods are\noften adequate. For model-free reinforcement learning, one can simply use Sarsa rather\nthan Q-learning. O\ufb00-policy methods free behavior from the target policy. This could be\nconsidered an appealing convenience but not a necessity. However, o\ufb00-policy learning is\nessential to other anticipated use cases, cases that we have not yet mentioned in this\nbook but may be important to the larger goal of creating a powerful intelligent agent. In these use cases, the agent learns not just a single value function and single policy,\nbut large numbers of them in parallel. There is extensive psychological evidence that\npeople and animals learn to predict many di\ufb00erent sensory events, not just rewards. We can be surprised by unusual events, and correct our predictions about them, even if\nthey are of neutral valence (neither good nor bad). This kind of prediction presumably\nunderlies predictive models of the world such as are used in planning. We predict what\nwe will see after eye movements, how long it will take to walk home, the probability of\nmaking a jump shot in basketball, and the satisfaction we will get from taking on a new\nproject. In all these cases, the events we would like to predict depend on our acting\nin a certain way. To learn them all, in parallel, requires learning from the one stream\nof experience. There are many target policies, and thus the one behavior policy cannot\nequal all of them. Yet parallel learning is conceptually possible because the behavior\npolicy may overlap in part with many of the target policies. To take full advantage of\nthis requires o\ufb00-policy learning.",
    "chapter_id": 147,
    "score": 0.5448455810546875,
    "page": 286
  },
  "6922": {
    "text": "ur own studies (Barto, Sutton, and Anderson, 1983;\nSutton, 1984). Michie consistently emphasized the role of trial and error and learning as\nessential aspects of arti\ufb01cial intelligence (Michie, 1974). Widrow, Gupta, and Maitra (1973) modi\ufb01ed the Least-Mean-Square (LMS) algorithm\nof Widrow and Ho\ufb00 (1960) to produce a reinforcement learning rule that could learn\nfrom success and failure signals instead of from training examples. They called this\nform of learning \u201cselective bootstrap adaptation\u201d and described it as \u201clearning with a\ncritic\u201d instead of \u201clearning with a teacher.\u201d They analyzed this rule and showed how it\ncould learn to play blackjack. This was an isolated foray into reinforcement learning by\nWidrow, whose contributions to supervised learning were much more in\ufb02uential. Our\nuse of the term \u201ccritic\u201d is derived from Widrow, Gupta, and Maitra\u2019s paper. Buchanan,\nMitchell, Smith, and Johnson (1978) independently used the term critic in the context\nof machine learning (see also Dietterich and Buchanan, 1984), but for them a critic is an\nexpert system able to do more than evaluate performance. Research on learning automata had a more direct in\ufb02uence on the trial-and-error\nthread leading to modern reinforcement learning research. These are methods for solv-\ning a nonassociative, purely selectional learning problem known as the k-armed bandit by\nanalogy to a slot machine, or \u201cone-armed bandit,\u201d except with k levers (see Chapter 2). Learning automata are simple, low-memory machines for improving the probability of\nreward in these problems. Learning automata originated with work in the 1960s of the\nRussian mathematician and physicist M. L. Tsetlin and colleagues (published posthu-\nmously in Tsetlin, 1973) and has been extensively developed since then within engineer-\ning (see Narendra and Thathachar, 1974, 1989). These developments included the study\nof stochastic learning automata, which are methods for updating action probabilities\non the basis of reward signals.",
    "chapter_id": 137,
    "score": 0.5090845227241516,
    "page": 39
  },
  "6929": {
    "text": "ing a distinct\nearly contribution to temporal-di\ufb00erence learning. The temporal-di\ufb00erence and optimal control threads were fully brought together in\n1989 with Chris Watkins\u2019s development of Q-learning. This work extended and inte-\ngrated prior work in all three threads of reinforcement learning research. Paul Werbos\n(1987) contributed to this integration by arguing for the convergence of trial-and-error\nlearning and dynamic programming since 1977. By the time of Watkins\u2019s work there\nhad been tremendous growth in reinforcement learning research, primarily in the ma-\nchine learning sub\ufb01eld of arti\ufb01cial intelligence, but also in neural networks and arti\ufb01cial\nintelligence more broadly. In 1992, the remarkable success of Gerry Tesauro\u2019s backgam-\nmon playing program, TD-Gammon, brought additional attention to the \ufb01eld. In the time since publication of the \ufb01rst edition of this book, a \ufb02ourishing sub\ufb01eld of\nneuroscience developed that focuses on the relationship between reinforcement learning\nalgorithms and reinforcement learning in the nervous system. Most responsible for this\n\n22\nChapter 1: Introduction\nis an uncanny similarity between the behavior of temporal-di\ufb00erence algorithms and\nthe activity of dopamine producing neurons in the brain, as pointed out by a number of\nresearchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\nDayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\nprovides an introduction to this exciting aspect of reinforcement learning. Other important contributions made in the recent history of reinforcement learning\nare too numerous to mention in this brief account; we cite many of these at the end of\nthe individual chapters in which they arise. Bibliographical Remarks\nFor additional general coverage of reinforcement learning, we refer the reader to the\nbooks by Szepesv\u00b4ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\nSugiyama, Hachiya, and Morimura (2013).",
    "chapter_id": 137,
    "score": 0.47408613562583923,
    "page": 43
  }
}